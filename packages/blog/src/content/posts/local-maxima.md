---
title: '능선 위에서'
pubDate: '2025-06-18'
description: '로컬 미니마 개념을 통해 삶의 방향과 최적화에 대해 생각해보기'
highlight: '결국 우리는 각자의 산등성이 위에서 다음 봉우리를 바라보고 있는 것뿐이다'
keywords: ['로컬미니마', '최적화', '방향', '경사하강법']
---

요즘 AI 덕분에 진짜 많은 게 가능해졌다. 개발자든 디자이너든 누구든 조금만 배우면 뭔가 만들어볼 수 있는 시대가 됐다. 나도 개발자로 살고 있지만, 이제는 개발자가 그냥 기술자라는 생각은 잘 안 든다. 어떤 면에서는 크리에이터에 더 가까워진 것 같다.

그런 환경 속에서 "나는 지금 어디로 가고 있는 걸까?"라는 질문을 자주 하게 된다. 특히 후배들을 멘토링하다 보면 더 그렇다. 기회는 넘치는데, 방향을 정하는 게 더 어려워졌으니까.

⸻

수학에는 **로컬 미니마local minima**라는 개념이 있다. 어떤 지점에서는 더 나아질 방향이 없어 보이지만, 전체로 보면 더 좋은 지점이 존재할 수 있다는 의미다. 경사 하강법 같은 방식으로 지금 위치에서 가장 나은 방향을 조금씩 찾아갈 수 있다. 이 방식은 실용적이지만 전체를 놓치는 한계도 분명하다. 더 나은 곳이 있다면, 지금 있는 자리에서, 또는 다른 시작점에서 움직여야 찾을 수 있다.

```python
x = initial_position
while not_converged:
    gradient = ∇f(x)
    x = x - learning_rate * gradient
```

코드로 보면 단순한데, 바로 이 단순함이 함정이다. 가장 가파른 길만 따라가다 보면 더 좋은 곳이 있어도 보지 못한 채 멈추거나 계속 진동할 수 있다. 이 방식은 효율적이지만, 반대로 전체를 놓치는 한계도 분명하다. 더 나은 곳이 있다면, 지금 있는 자리에서 일단 내려와야 다시 찾을 수 있다.

⸻

삶도 비슷하게 움직이는 것 같다. 어느 정도 안정되고, 성과도 있고, 나쁘지 않다고 느껴지지만, 가끔은 "이게 전부인가?"라는 생각이 스친다. 5년차 개발자로 살면서, 프론트엔드에서 백엔드로 역할을 확장하는 시기인 지금이 그런 순간인 것 같다. 익숙한 영역에서 벗어나 새로운 산을 오르기 시작한 느낌이다.

그럴 땐 내가 이 방향을 계속 가야 할지, 아니면 전혀 다른 방향을 다시 생각해봐야 할지 고민이 된다. 그래서 로컬 미니마를 벗어나는 몇 가지 전략이 있다는 게 흥미롭다.

**랜덤 리스타트(Random Restart)**: 여러 무작위 시작점에서 탐색을 시작해 전역 최적해를 찾는 방법. 한 방향에서만 답을 찾지 말고, 아예 다른 곳에서 다시 시작해본다.

**시뮬레이티드 어닐링(Simulated Annealing)**: 높은 온도에서 시작해 점진적으로 온도를 낮추며, 초기에는 나쁜 해도 확률적으로 수용하는 방법. 지금보다 나빠 보여도, 일단 한번 가본다.

**유전 알고리즘(Genetic Algorithm)**: 자연선택과 유전 원리를 모방해 해집단을 진화시키는 방법. 좋은 해답들을 서로 교차(crossover)하고 변이(mutation)시켜 예상 밖의 조합을 시험한다.

**Adam 옵티마이저(Adaptive Moment Estimation)**: 각 매개변수에 대해 적응적 학습률을 사용하며, 과거 기울기의 1차 및 2차 모멘트를 추정하는 방법. 방향도, 속도도, 상황에 따라 적응적으로 바꾼다.

어디선가 들어본 듯한 접근방식들이지 않은가?

⸻

카뮈는 부조리 속에서도 계속 살아나가자 한다. 우리는 계속 나아갈 테지만, 모든 사람이 같은 가장 높은 봉우리를 향해 가는 건 아니다. 누군가는 넓게 돌고, 누군가는 높게 오른다. 누군가는 자주 멈추고, 누군가는 계속 걷는다. 누군가는 즐겁게. 그래서 굳이 최적화를 강박처럼 붙들지 않아도 된다. 지금 이 방향이, 이 속도가 나에게 의미가 있다면 그걸로 충분할 수 있다.

결국 우리는 각자의 산등성이 위에서 다음 봉우리를 바라보고 있는 것뿐이다. 가끔은 내려와야 하고, 가끔은 그대로 머물러도 된다.

⸻

**참고한 것들**

- 장기하와 얼굴들, 「등산은 왜할까」 (mono, 2018)
- 3Blue1Brown, [「Gradient descent, how neural networks learn」](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- 뉴욕털게, [「최적화의 숨겨진 비용」](https://youtu.be/aB58_Z7ShT4?si=tjbvVlKwDSeAifGD)
