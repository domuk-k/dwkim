#!/usr/bin/env tsx

import fs from 'node:fs/promises'
import path from 'node:path'
import { NeonPostgres } from '@langchain/community/vectorstores/neon'
import { Document as LangChainDocument } from '@langchain/core/documents'
import { GoogleGenerativeAIEmbeddings } from '@langchain/google-genai'
import dotenv from 'dotenv'

// Load .env.local first (for local dev), then .env
dotenv.config({ path: '.env.local' })
dotenv.config()

// Configuration
const DATA_DIR = path.join(__dirname, '../data')
const BLOG_ABOUT_DIR = path.join(__dirname, '../../blog/src/content/about')
const BLOG_POSTS_DIR = path.join(__dirname, '../../blog/src/content/posts')

interface ChunkResult {
  id: string
  content: string
  metadata: {
    type: string
    title: string
    category?: string
    source: 'persona-api' | 'blog'
    pubDate?: string
    keywords?: string[]
    chunkIndex: number
    totalChunks: number
  }
}

/**
 * ë‹¨ë½ ê¸°ë°˜ ì²­í‚¹ (persona-api/dataìš©)
 */
function chunkByParagraph(text: string, maxSize: number = 1000): string[] {
  const paragraphs = text.split('\n\n').filter((p) => p.trim().length > 0)
  const chunks: string[] = []
  let currentChunk = ''

  for (const paragraph of paragraphs) {
    if (currentChunk.length + paragraph.length <= maxSize) {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph
    } else {
      if (currentChunk) chunks.push(currentChunk)
      currentChunk = paragraph
    }
  }
  if (currentChunk) chunks.push(currentChunk)
  return chunks
}

/**
 * ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹ (blog postsìš© - H2 ê¸°ì¤€)
 */
function chunkBySection(text: string): string[] {
  const sections = text.split(/(?=^## )/m)
  return sections.filter((s) => s.trim().length > 0)
}

/**
 * YAML Frontmatter íŒŒì‹±
 */
function parseFrontmatter(content: string): {
  frontmatter: Record<string, unknown>
  body: string
} {
  const match = content.match(/^---\n([\s\S]*?)\n---\n([\s\S]*)$/)
  if (!match) return { frontmatter: {}, body: content }

  const frontmatter: Record<string, unknown> = {}
  match[1].split('\n').forEach((line) => {
    const colonIndex = line.indexOf(':')
    if (colonIndex === -1) return

    const key = line.slice(0, colonIndex).trim()
    let value: unknown = line.slice(colonIndex + 1).trim()

    // ë°°ì—´ íŒŒì‹± (keywords ë“±)
    if (typeof value === 'string' && value.startsWith('[')) {
      try {
        value = JSON.parse(value.replace(/'/g, '"'))
      } catch {
        // íŒŒì‹± ì‹¤íŒ¨ì‹œ ê·¸ëŒ€ë¡œ ë¬¸ìì—´
      }
    } else if (typeof value === 'string' && (value.startsWith("'") || value.startsWith('"'))) {
      value = value.slice(1, -1)
    }

    frontmatter[key] = value
  })

  return { frontmatter, body: match[2] }
}

/**
 * í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ ì œëª© ì¶”ì¶œ
 */
function extractTitle(text: string): string | null {
  const match = text.match(/^#\s+(.+)$/m)
  return match ? match[1] : null
}

/**
 * í‚¤ì›Œë“œë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
 */
function inferCategory(keywords?: string[]): string {
  if (!keywords || keywords.length === 0) return 'general'

  const aiKeywords = ['AI', 'Claude', 'agent', 'LLM', 'RAG']
  const devKeywords = ['í”„ë¡œì íŠ¸', 'ë©˜íƒˆëª¨ë¸', 'í”„ë¡œì„¸ìŠ¤', 'TDD']

  if (keywords.some((k) => aiKeywords.some((ai) => k.includes(ai)))) return 'ai'
  if (keywords.some((k) => devKeywords.some((dev) => k.includes(dev)))) return 'methodology'
  return 'philosophy'
}

/**
 * persona-api/data/*.md ì²˜ë¦¬
 */
async function processDataFiles(): Promise<ChunkResult[]> {
  console.log('ğŸ“‚ Processing persona-api/data files...')

  let files: string[]
  try {
    files = await fs.readdir(DATA_DIR)
  } catch {
    console.warn('âš ï¸  data ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:', DATA_DIR)
    return []
  }

  const results: ChunkResult[] = []
  const mdFiles = files.filter((f) => f.endsWith('.md') && f !== 'systemPrompt.md')

  for (const file of mdFiles) {
    const content = await fs.readFile(path.join(DATA_DIR, file), 'utf-8')
    const type = path.basename(file, '.md')

    // resume.mdëŠ” ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
    if (type === 'resume') {
      console.log(`  ğŸ“„ ${file}: 1 chunk (whole document)`)
      results.push({
        id: `data_${type}_0`,
        content: content,
        metadata: {
          type,
          title: extractTitle(content) || 'ê¹€ë™ìš± ì´ë ¥ì„œ',
          source: 'persona-api',
          chunkIndex: 0,
          totalChunks: 1
        }
      })
      continue
    }

    const chunks = chunkByParagraph(content)
    console.log(`  ğŸ“„ ${file}: ${chunks.length} chunks`)

    chunks.forEach((chunk, index) => {
      results.push({
        id: `data_${type}_${index}`,
        content: chunk,
        metadata: {
          type,
          title: extractTitle(chunk) || type,
          source: 'persona-api',
          chunkIndex: index,
          totalChunks: chunks.length
        }
      })
    })
  }

  return results
}

/**
 * blog/src/content/about/*.md ì²˜ë¦¬
 */
async function processAboutFiles(): Promise<ChunkResult[]> {
  console.log('ğŸ“‚ Processing blog/about files...')

  let files: string[]
  try {
    files = await fs.readdir(BLOG_ABOUT_DIR)
  } catch {
    console.warn('âš ï¸  about ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:', BLOG_ABOUT_DIR)
    return []
  }

  const results: ChunkResult[] = []
  const mdFiles = files.filter((f) => f.endsWith('.md'))

  for (const file of mdFiles) {
    const content = await fs.readFile(path.join(BLOG_ABOUT_DIR, file), 'utf-8')
    const { frontmatter, body } = parseFrontmatter(content)

    // rag: falseì¸ ê²½ìš° ìŠ¤í‚µ
    if (frontmatter.rag === false || frontmatter.rag === 'false') {
      console.log(`  â­ï¸  ${file}: skipped (rag: false)`)
      continue
    }

    const category = path.basename(file, '.md')

    console.log(`  ğŸ“„ ${file}: 1 chunk (whole document)`)

    // About íŒŒì¼ì€ ì§§ìœ¼ë¯€ë¡œ ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
    results.push({
      id: `about_${category}_0`,
      content: body.trim(),
      metadata: {
        type: 'about',
        title: (frontmatter.title as string) || category,
        category,
        source: 'blog',
        chunkIndex: 0,
        totalChunks: 1
      }
    })
  }

  return results
}

/**
 * blog/src/content/posts/*.md ì²˜ë¦¬
 */
async function processPostFiles(): Promise<ChunkResult[]> {
  console.log('ğŸ“‚ Processing blog/posts files...')

  let files: string[]
  try {
    files = await fs.readdir(BLOG_POSTS_DIR)
  } catch {
    console.warn('âš ï¸  posts ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:', BLOG_POSTS_DIR)
    return []
  }

  const results: ChunkResult[] = []
  const mdFiles = files.filter((f) => f.endsWith('.md'))

  for (const file of mdFiles) {
    const content = await fs.readFile(path.join(BLOG_POSTS_DIR, file), 'utf-8')
    const { frontmatter, body } = parseFrontmatter(content)

    // rag: falseì¸ ê²½ìš° ìŠ¤í‚µ
    if (frontmatter.rag === false || frontmatter.rag === 'false') {
      console.log(`  â­ï¸  ${file}: skipped (rag: false)`)
      continue
    }

    const slug = path.basename(file, '.md')
    const keywords = frontmatter.keywords as string[] | undefined

    // ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹
    const chunks = chunkBySection(body)

    console.log(`  ğŸ“„ ${file}: ${chunks.length} chunks`)

    chunks.forEach((chunk, index) => {
      results.push({
        id: `post_${slug}_${index}`,
        content: chunk.trim(),
        metadata: {
          type: 'post',
          title: (frontmatter.title as string) || slug,
          category: inferCategory(keywords),
          source: 'blog',
          pubDate: frontmatter.pubDate as string,
          keywords,
          chunkIndex: index,
          totalChunks: chunks.length
        }
      })
    })
  }

  return results
}

/**
 * Neon DB ì´ˆê¸°í™”
 */
async function initializeDatabase(testMode: boolean = false) {
  console.log('\nğŸš€ Neon DB ì´ˆê¸°í™” ì‹œì‘...\n')

  const connectionString = process.env.DATABASE_URL
  const apiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY

  // í…ŒìŠ¤íŠ¸ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ í™˜ê²½ë³€ìˆ˜ ì²´í¬
  if (!testMode) {
    if (!connectionString) {
      console.error('âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤')
      process.exit(1)
    }

    if (!apiKey) {
      console.error('âŒ GOOGLE_API_KEY ë˜ëŠ” GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤')
      process.exit(1)
    }
  }

  // ëª¨ë“  ë¬¸ì„œ ìˆ˜ì§‘
  const dataChunks = await processDataFiles()
  const aboutChunks = await processAboutFiles()
  const postChunks = await processPostFiles()

  const allChunks = [...dataChunks, ...aboutChunks, ...postChunks]

  console.log(`\nğŸ“Š ì´ ì²­í¬ ìˆ˜: ${allChunks.length}`)
  console.log(`   - data: ${dataChunks.length}`)
  console.log(`   - about: ${aboutChunks.length}`)
  console.log(`   - posts: ${postChunks.length}\n`)

  if (testMode) {
    console.log('ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ - DB ì—…ë¡œë“œ ê±´ë„ˆëœ€\n')
    console.log('ìƒ˜í”Œ ì²­í¬:')
    allChunks.slice(0, 3).forEach((chunk, i) => {
      console.log(`\n--- Chunk ${i + 1} ---`)
      console.log(`ID: ${chunk.id}`)
      console.log(`Type: ${chunk.metadata.type}`)
      console.log(`Title: ${chunk.metadata.title}`)
      console.log(`Content: ${chunk.content.substring(0, 100)}...`)
    })
    return
  }

  // Embeddings ì´ˆê¸°í™”
  console.log('ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...')
  const embeddings = new GoogleGenerativeAIEmbeddings({
    apiKey,
    model: 'text-embedding-004'
  })

  // Vector Store ì´ˆê¸°í™”
  console.log('ğŸ”§ Vector Store ì—°ê²°...')
  const vectorStore = await NeonPostgres.initialize(embeddings, {
    connectionString,
    tableName: 'persona_documents',
    columns: {
      idColumnName: 'id',
      vectorColumnName: 'embedding',
      contentColumnName: 'content',
      metadataColumnName: 'metadata'
    }
  })

  // ë°°ì¹˜ë¡œ ë¬¸ì„œ ì¶”ê°€
  const batchSize = 10
  for (let i = 0; i < allChunks.length; i += batchSize) {
    const batch = allChunks.slice(i, i + batchSize)
    const batchNum = Math.floor(i / batchSize) + 1
    const totalBatches = Math.ceil(allChunks.length / batchSize)

    console.log(`ğŸ“¤ ë°°ì¹˜ ì—…ë¡œë“œ ${batchNum}/${totalBatches}...`)

    const docs = batch.map(
      (chunk) =>
        new LangChainDocument({
          pageContent: chunk.content,
          metadata: {
            ...chunk.metadata,
            docId: chunk.id
          }
        })
    )

    // NeonPostgres expects UUID, so let it auto-generate IDs
    // We store our custom ID in metadata.docId instead
    await vectorStore.addDocuments(docs)
  }

  console.log('\nâœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!')
}

/**
 * ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
 */
async function testRetrieval() {
  console.log('\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n')

  const connectionString = process.env.DATABASE_URL
  const apiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY

  if (!connectionString || !apiKey) {
    console.error('âŒ DATABASE_URLê³¼ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤')
    return
  }

  const embeddings = new GoogleGenerativeAIEmbeddings({
    apiKey,
    model: 'text-embedding-004'
  })

  const vectorStore = await NeonPostgres.initialize(embeddings, {
    connectionString,
    tableName: 'persona_documents',
    columns: {
      idColumnName: 'id',
      vectorColumnName: 'embedding',
      contentColumnName: 'content',
      metadataColumnName: 'metadata'
    }
  })

  const testQueries = [
    'ê¸°ìˆ  ìŠ¤íƒì´ ë­”ê°€ìš”?',
    'ì–´ë–¤ í”„ë¡œì íŠ¸ë¥¼ í–ˆë‚˜ìš”?',
    'AIì— ëŒ€í•œ ìƒê°ì€?',
    'ê°œë°œ ì² í•™ì´ ìˆë‚˜ìš”?'
  ]

  for (const query of testQueries) {
    console.log(`â“ Query: "${query}"`)

    const results = await vectorStore.similaritySearch(query, 3)

    if (results.length === 0) {
      console.log('   âŒ ê²°ê³¼ ì—†ìŒ\n')
    } else {
      results.forEach((doc, index) => {
        console.log(
          `   ${index + 1}. [${doc.metadata.type}] ${doc.pageContent.substring(0, 80)}...`
        )
      })
      console.log('')
    }
  }
}

/**
 * DB í´ë¦° (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ)
 */
async function cleanDatabase() {
  console.log('\nğŸ§¹ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘...')

  const connectionString = process.env.DATABASE_URL

  if (!connectionString) {
    console.error('âŒ DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤')
    process.exit(1)
  }

  // Neon serverless driver ì‚¬ìš©
  const { neon } = await import('@neondatabase/serverless')
  const sql = neon(connectionString)

  try {
    await sql`DELETE FROM persona_documents`
    console.log('âœ… persona_documents í…Œì´ë¸” í´ë¦¬ì–´ ì™„ë£Œ')
  } catch (error) {
    console.log('âš ï¸  í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰):', error)
  }
}

// Main
async function main() {
  const args = process.argv.slice(2)
  const testMode = args.includes('--test')
  const runRetrieval = args.includes('--retrieval')
  const cleanMode = args.includes('--clean')

  if (cleanMode) {
    await cleanDatabase()
  }

  if (runRetrieval) {
    await testRetrieval()
  } else {
    await initializeDatabase(testMode)

    if (!testMode) {
      await testRetrieval()
    }
  }

  console.log('\nğŸ‰ ì™„ë£Œ!\n')
}

main().catch((error) => {
  console.error('âŒ ì˜¤ë¥˜ ë°œìƒ:', error)
  process.exit(1)
})
